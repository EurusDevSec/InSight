"""
Ki·ªÉm tra t√≠nh to√†n v·∫πn c·ªßa dataset
Ch·∫°y: python scripts/validate_dataset.py [--path <path_to_ground_truth>]
"""

import json
import argparse
from pathlib import Path


def validate_dataset(gt_path: str = "data/annotations/ground_truth.json"):
    errors = []
    warnings = []

    # Load ground truth
    gt_file = Path(gt_path)
    if not gt_file.exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y {gt_path}")
        return False
    # mo ft_file
    with open(gt_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    samples = data["samples"]
    base_dir = gt_file.parent.parent  # data/poc/ ho·∫∑c data/

    print(f"üìä Ki·ªÉm tra {len(samples)} m·∫´u t·ª´ {gt_path}...\n")

    # Th·ªëng k√™ theo category
    categories = {}

    for sample in samples:
        sample_id = sample["id"]
        category = sample.get("food_category", "unknown")
        categories[category] = categories.get(category, 0) + 1

        # 1. Ki·ªÉm tra ·∫£nh t·ªìn t·∫°i
        img_path = base_dir / sample["image_file"]
        if not img_path.exists():
            errors.append(f"[{sample_id}] Thi·∫øu ·∫£nh: {img_path}")

        # 2. Ki·ªÉm tra ground truth h·ª£p l·ªá
        gt = sample["ground_truth"]
        if gt["total_weight_g"] <= 0:
            errors.append(f"[{sample_id}] Tr·ªçng l∆∞·ª£ng <= 0")

        if gt["is_liquid"] and gt["liquid_volume_ml"] <= 0:
            warnings.append(f"[{sample_id}] M√≥n n∆∞·ªõc nh∆∞ng kh√¥ng c√≥ th·ªÉ t√≠ch n∆∞·ªõc")

        # 3. Ki·ªÉm tra metadata
        if not sample["metadata"].get("restaurant"):
            warnings.append(f"[{sample_id}] Thi·∫øu th√¥ng tin qu√°n")

        # 4. Ki·ªÉm tra image file extension
        img_file = sample["image_file"]
        if not img_file.lower().endswith((".jpg", ".jpeg", ".png")):
            warnings.append(f"[{sample_id}] File kh√¥ng ph·∫£i ·∫£nh: {img_file}")

    # Report
    print("=" * 50)

    # Th·ªëng k√™ category
    print("üìä Th·ªëng k√™ theo lo·∫°i:")
    for cat, count in sorted(categories.items()):
        print(f"   - {cat}: {count} m·∫´u")
    print()

    if errors:
        print(f"‚ùå {len(errors)} L·ªñI:")
        for e in errors:
            print(f"   - {e}")
    else:
        print("‚úÖ Kh√¥ng c√≥ l·ªói nghi√™m tr·ªçng!")

    if warnings:
        print(f"\n‚ö†Ô∏è  {len(warnings)} C·∫¢NH B√ÅO:")
        for w in warnings:
            print(f"   - {w}")

    print("=" * 50)
    valid = len(samples) - len(errors)
    print(f"üìà K·∫øt qu·∫£: {valid}/{len(samples)} m·∫´u h·ª£p l·ªá")

    return len(errors) == 0


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Validate InSight dataset")
    parser.add_argument(
        "--path",
        default="data/annotations/ground_truth.json",
        help="Path to ground_truth.json",
    )
    args = parser.parse_args()

    success = validate_dataset(args.path)
    exit(0 if success else 1)
